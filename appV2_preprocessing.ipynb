{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run appV2_dataCleaning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: RE manipulation needs to be done with SME (aka Amar) \n",
    "tidy up, uniform data by using regular expression based on data exploration results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run appV2_data_exploration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: model selection\n",
    "a Bag of Words & tf-idf techniques to extract features from each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the comment text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in tokens]\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "#df_binary = df.copy()\n",
    "X = df['final']\n",
    "y = df['result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn's tfidf\n",
    "#Text preprocessing, tokenizing and filtering out stopwords, which builds a dictionary of features and transforms documents to\n",
    "#feature vectors: from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = TfidfVectorizer(tokenizer= lemma, min_df=5, lowercase = True, ngram_range=(1,2), stop_words='english').fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized.max(0).toarray().ravel().argsort()\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(C=10)\n",
    "text_clf =clf.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "class_names = np.array(['Negative', 'Positive','Intermediate'])\n",
    "mx = plot_confusion_matrix(text_clf, X_test_vectorized, y_test, display_labels=class_names,  cmap=plt.cm.Blues,normalize=None, xticks_rotation= 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_test_vectorized)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "#print('Precision: {:.2f}'.format(precision_score(y_test, predictions,  average='micro')))\n",
    "#print('Recall: {:.2f}'.format(recall_score(y_test, predictions,  average='micro')))\n",
    "#average ='macro' for small classes in imbalanced data\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, predictions, average='macro')))\n",
    "target_names = ['Negative', 'Positive', 'Intermediate']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Multi-Class Classifier: Features and Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes classifier: the one most suitable for word count is the multinomial variant \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vect.transform(['Bone left foot third metatarsal biopsy Acute osteomyelitis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)  \n",
    "X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "ready to experiment different ml models and evaluate their accuracy. \n",
    "we will benchmark the following 6 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(multi_class='ovr'),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(multi_class='ovr', random_state=0),\n",
    "    lgb.LGBMClassifier(objective='multiclass'),\n",
    "    xgb.XGBClassifier(objective= 'multi:softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate metric for each label, find average weight by support number of true instances for each label. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    f1 = cross_val_score(model, features, labels, scoring='f1_macro', cv=CV) \n",
    "    print(f1)\n",
    "    for fold_idx, fv in enumerate(f1):\n",
    "        entries.append((model_name, fold_idx, fv))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "sns.boxplot(x='model_name', y='f1_macro', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='f1_macro', data=cv_df, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').f1_macro.mean().sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(objective='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.20, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "target_names = ['Negative', 'Positive', 'Intermediate']\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',  xticklabels=target_names, yticklabels=target_names) #cmap=plt.cm.Blues,\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the vast majority of predictions end up on the diagonal,which we expected. However, there are misclassifications, and it might be \n",
    "#interesting to see what those are caused by\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, predicted in interpretation_to_id.items():\n",
    "    for _, actual in interpretation_to_id.items():\n",
    "        if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "            print(f\"'{id_to_interpretation[actual]}' predicted as '{id_to_interpretation[predicted]}' : {conf_mat[actual, predicted]} examples.\")\n",
    "            #display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['interpretation', 'comment']])\n",
    "            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['interpretation', 'final']].style.set_properties(subset=['final'], **{'width': '800px'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My way:difference btw y_test and y_pred and grab their index \n",
    "temp=y_test != y_pred\n",
    "temp[temp == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate the error records by using the index above\n",
    "df_error = df.loc[temp[temp == True].index]\n",
    "#In order to add the pred value to the df_error, need to convert the y_pred from array to series first. \n",
    "pred = pd.Series(y_pred, index=y_test.index, name='pred')\n",
    "#then subset the error prediction but using the index above\n",
    "pred= pred[temp[temp == True].index]\n",
    "#add the prediction column to the error dataframe\n",
    "df_error['pred'] = pred\n",
    "df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective= 'multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Utility.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_mat = np.array(pd.crosstab(y_test, y_pred))\n",
    "#conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, predicted in interpretation_to_id.items():\n",
    "    for _, actual in interpretation_to_id.items():\n",
    "        if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "            print(f\"'{id_to_interpretation[actual]}' predicted as '{id_to_interpretation[predicted]}' : {conf_mat[actual, predicted]} examples.\")\n",
    "            #display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['interpretation', 'comment']])\n",
    "            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['interpretation', 'final']].style.set_properties(subset=['final'], **{'width': '800px'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search param dict for xgboost and light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_pos_weight = number of negative samples / number of positive samples\n",
    "#learning rate: default = 0.1\n",
    "#num_iterations : def=100  aliases: n_estimators, num_iteration\n",
    "#num_leaves: def=31 max leaves for each trained tree. Typical: 255, usually {15, 31, 63, 127, 255, 511, 1023, 2047, 4095}.\n",
    "#max_depth: def=-1 typical 6 [3, 12]\n",
    "#min_data_in_leaf: 20 typical 100 min_child_samples\n",
    "#min_sum_hessian_in_leaf: def= 0.001 min_child_weight\n",
    "#max_bin: def= 255 \n",
    "param_dict={'objective': ['multiclass'], \n",
    "            'is_unbalance': ['True'],\n",
    "            'metric': ['AUC-mu']\n",
    "            'scale_pos_weight': [],\n",
    "            'learning_rate': [],\n",
    "            'num_iterations': [],\n",
    "            'num_leaves': [],\n",
    "            'min_data_in_leaf': [],\n",
    "            'min_child_weight': [],\n",
    "            'max_bin':[],\n",
    "            'max_depth':[]\n",
    "                        \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.01, 0.6, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict={'objective': ['multiclass'], \n",
    "            'is_unbalance': ['True']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = lgb.LGBMClassifier()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.20, random_state=0)\n",
    "def paramTuning(param_name, param_value_ls):\n",
    "    param_dict[param_name] = param_value_ls\n",
    "    gridsearch = GridSearchCV(model, param_dict, cv=5, scoring='f1_macro') \n",
    "    #lgb_model = gridsearch.fit(features, labels)\n",
    "    lgb_model = gridsearch.fit(X_train, y_train)\n",
    "    best_index = gridsearch.best_index_\n",
    "    best_param_value = gridsearch.cv_results_['params'][best_index][param_name]\n",
    "    plt.plot(param_value_ls, gridsearch.cv_results_['mean_test_score'], label='f1_macro')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('f1_macro')\n",
    "    plt.title('lightgbm ' + param_name + ' vs f1_macro')\n",
    "    plt.legend(loc=\"best\") \n",
    "    plt.show()\n",
    "    print('Best', param_name, '=', best_param_value, '| Best f1_macro = ', gridsearch.best_score_)\n",
    "    return [best_param_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['learning_rate'] =paramTuning('learning_rate', np.arange(0.01, 0.6, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([50, 75])\n",
    "arr2 = np.arange(100, 300, 50)\n",
    "arr = np.concatenate((arr1, arr2))\n",
    "#n_estimators \n",
    "param_dict['num_iterations'] =paramTuning('num_iterations',  np.array([75, 100, 125, 150, 200, 300, 400, 500, 600]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['num_leaves'] =paramTuning('num_leaves', np.array([15, 31, 63, 127, 255, 511, 1023, 2047, 4095]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['max_depth'] =paramTuning('max_depth', np.array([1, 3, 5, 7, 10, 15, 20, 25, 30, 35, 40, 45]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_child_samples\n",
    "param_dict['min_data_in_leaf'] =paramTuning('min_data_in_leaf', np.array([5, 10, 15, 20, 25, 30, 35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_child_weight\n",
    "param_dict['min_sum_hessian_in_leaf'] =paramTuning('min_sum_hessian_in_leaf', np.array([0.0001, 0.0005, 0.001, 0.002, 0.003, 0.004, 0.005]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['max_bin'] =paramTuning('max_bin', np.array([100, 200, 240, 255, 300, 350, 400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#min_data_in_leaf, max_depth, do not mess around with min_sum_hessian_in_leaf (seems 0.0001 works)\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict=param_dict={'objective': ['multiclass'],  'is_unbalance': ['True'], 'num_iterations': [300], 'learning_rate': [0.4],'max_depth': [25], 'num_leaves': [31], 'min_data_in_leaf': [20], 'max_bin': [255] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = lgb.LGBMClassifier(objective= 'objective', is_unbalance=True, num_iterations=75, learning_rate=0.4, max_dept=9, min_data_in_leaf=5, min_sum_hessian_in_leaf= 0.0001, scale_pos_weight=0.1)\n",
    "#model.fit(X_train, y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.20, random_state=0)\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5)) \n",
    "target_names = ['Negative', 'Positive', 'Intermediate']\n",
    "res=sns.heatmap(conf_mat, annot=True, fmt='d', cmap=plt.cm.Blues, xticklabels=target_names, yticklabels=target_names) #cmap=plt.cm.Blues,\n",
    "for _, spine in res.spines.items():\n",
    "    spine.set_visible(True)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptunecontrib.monitoring.utils import axes2fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
