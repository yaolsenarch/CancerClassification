{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run appV2_dataCleaning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting tip:if chain format like tfidf= TfidfVectorizer(...).fit_transform() then \n",
    "#tfidf.get_feature_names doesnt work. However, if without chain like tfidf= TfidfVectorizer(...)\n",
    "#tfidf.fit_transform(), then tfidf.get_feature_names works.\n",
    "\n",
    "#conventional algor are often baised towards the majoirty class, not taking the data distribution into \n",
    "#consideration. One solution is to artificially balance the dataset for example oversampling each class. \n",
    "\n",
    "#classifier+learning algorithms can not directly process the text documents in their original form, \n",
    "#as most of them expect numerical feature vector with a fixed size rather than the raw text documents with\n",
    "#variable length. One common approach for extracting features from text is to use the bag of words model:\n",
    "#where for each document the presence(and often the freq) of words is taken into consideration. \n",
    "#Specifically, for each term in our dataset, we will calculate a measure called Term Freq Inverse Document \n",
    "#Freq (tf-idf) to calculate a tf-idf vector for each of comments.  \n",
    "#tfidf = TfidfVectorizer(tokenizer= lemma, min_df=5, lowercase = True, ngram_range=(1,2), stop_words='english')\n",
    "#vect_vectorized = tfidf.fit_transform(df['comment']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lemmatize the comment text instead of porter stemming\n",
    "import pyforest\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "def lemma(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "tfidf=TfidfVectorizer(tokenizer= lemma, min_df=3, lowercase = True, ngram_range=(1,2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.final).toarray()\n",
    "labels = df.result\n",
    "features.shape\n",
    "\n",
    "interpretation_id_df = df[['interpretation', 'result']].sort_values('result')\n",
    "interpretation_to_id = dict(interpretation_id_df.values)\n",
    "id_to_interpretation = dict(interpretation_id_df[['result', 'interpretation']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(features.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each comment is represented by 3956 features, representing the tf-idf score for different unigrams and bigrams. \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interpretation, result in interpretation_to_id.items():\n",
    "    print(interpretation, ' ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[labels==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "features_chi2 = chi2(features, labels==0) \n",
    "score = features_chi2[0]\n",
    "score = tuple(score)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = tuple(tfidf.get_feature_names())\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, chi2) in enumerate(zip(tfidf.get_feature_names(), score)): \n",
    "    #if name == 'situ':\n",
    "    print(i, '', name, '', chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len([v for v in features_chi2[0] if np.isnan(v) == False]) \n",
    "idx = np.argsort(features_chi2[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf.get_feature_names()[0]\n",
    "lst = np.array(tfidf.get_feature_names())[idx]  \n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v for v in lst if len(v.split(' ')) == 1 ] #and v.split(' ')== 'situ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chi2 to find the unigram/bigram terms that are the most correlated with each of the category:\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "N = 20\n",
    "for interpretation, result in interpretation_to_id.items():\n",
    "    features_chi2 = chi2(features, labels==result) #return chi2 and p values\n",
    "    #indices = np.asarray(features_chi2).max(0).argsort() \n",
    "    indices = np.argsort(features_chi2[0])#[::-1]\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]#tfidf has been fit and transformed.\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(f'# {interpretation}') \n",
    "    ugrams = '\\n. '.join(unigrams[-N:])\n",
    "    print(f'  @@ Most correlated unigrams:\\n. {ugrams}')   \n",
    "    bgrams = '\\n. '.join(bigrams[-N:])\n",
    "    print(f'  @@ Most correlated bigrams:\\n. {bgrams}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name features: top 20 features that have the largest tf-idf\n",
    "classifier+learning algorithms can not directly process the text documents in their original form, as most of them expect numerical feature vector with a fixed size rather than the raw text documents with variable length. One common approach for extracting features from text is to use the bag of words model: where for each document the presence(and often the freq) of words is taken into consideration. Specifically, for each term in our dataset, we will calculate a measure called Term Freq Inverse Document Freq (tf-idf) to calculate a tf-idf vector for each of comments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer= lemma, min_df=3, lowercase = True, ngram_range=(1,2), stop_words='english')\n",
    "vect_vectorized = tfidf.fit_transform(df['final']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tfidf_index = features.max(0).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a bag names\n",
    "feature_names = np.array(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#features now are all in a matrix. find max value along the rows. \n",
    "names = pd.Series(feature_names[vect_vectorized.max(0).toarray().ravel().argsort()], name = 'feature_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tfidf_value = pd.Series(sorted(vect_vectorized.max(0).toarray().ravel().tolist()), name='tfidf', index = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect_df = pd.concat([names, sorted_tfidf_value], axis=1)\n",
    "#vect_df.set_index('feature_names') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_tfidf_value.nlargest(20) # katherine needs to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[names.str.contains('esophagu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tfidf_value[sorted_tfidf_value.index.str.contains('esophagu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_tfidf_value.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historical code:\n",
    "vect = TfidfVectorizer(lowercase = True, stop_words = 'english', ngram_range=(2,5) ).fit(df['comment'])\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "#transform the document into a do\n",
    "vect_vectorized = vect.transform(df['comment']) \n",
    "sorted_tfidf_index = vect_vectorized.max(0).toarray().ravel().argsort()\n",
    "names = pd.Series(feature_names[vect_vectorized.max(0).toarray().ravel().argsort()], name = 'feature_names')\n",
    "sorted_tfidf_value = pd.Series(sorted(vect_vectorized.max(0).toarray().ravel().tolist()), name='tfidf')\n",
    "vect_df = pd.concat([names, sorted_tfidf_value], axis=1)\n",
    "small_df= vect_df.sort_values(by=['tfidf', 'feature_names']).iloc[:20,]\n",
    "small_s=small_df['tfidf']\n",
    "small_s.index=small_df['feature_names']\n",
    "small_s.name=None\n",
    "small_s.index.name=None\n",
    "small_s\n",
    "large_df = vect_df.sort_values(by=['tfidf', 'feature_names'], ascending = [False, True]).iloc[:20,] \n",
    "large_s = large_df['tfidf']\n",
    "large_s.index=large_df['feature_names']\n",
    "large_s.name = None\n",
    "large_s.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tfidf_value = pd.Series(sorted(vect_vectorized.max(0).toarray().ravel().tolist()), name='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_df = pd.concat([names, sorted_tfidf_value], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df = vect_df.sort_values(by=['tfidf', 'feature_names'], ascending = [False, True]).iloc[:20,] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k = range(2, 100)[::5]\n",
    "test_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
